{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16297638-f8bd-4b24-8128-406a2a4997f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd1291-a256-472c-8b3a-49bdbac56a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_google_vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5241fcc-42da-4457-a789-65a2ee477000",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627145b-2a83-4977-b5ce-0835564e9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a3f85-8ec5-4372-bfb7-6e5bf4714437",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c14deb-0522-4eba-9814-8d51783ef3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unstructured[all-docs]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a7f69-455e-442b-a850-8a69fc1fd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow-heif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4cdd98-1531-46f1-8d47-3c5f7a4f368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1b5e1-d5c8-4a14-a1ba-89fb3c540db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract\n",
    "!pip install tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40e677-b677-47c5-a246-bdf085c8552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c804fca-90d5-4c3c-be78-54590d9d3dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting json-repair\n",
      "  Downloading json_repair-0.28.3-py3-none-any.whl.metadata (8.0 kB)\n",
      "Downloading json_repair-0.28.3-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: json-repair\n",
      "Successfully installed json-repair-0.28.3\n"
     ]
    }
   ],
   "source": [
    "!pip install json-repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5005fb77-cc6f-4c21-a22f-c66fbb3942f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from langchain_google_vertexai import (\n",
    "    VertexAI,\n",
    "    ChatVertexAI,\n",
    "    VertexAIEmbeddings,\n",
    "    VectorSearchVectorStore,\n",
    ")\n",
    "from google.cloud import aiplatform\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain import LLMChain\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from neo4j.exceptions import ClientError\n",
    "import os\n",
    "from vertexai.generative_models import GenerativeModel, Image\n",
    "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
    "from langchain_google_vertexai import ChatVertexAI, HarmBlockThreshold, HarmCategory\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing import List, Tuple\n",
    "import base64\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.graphs.graph_document import GraphDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7443ea29-e5b8-4dde-8424-94be3dc06c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"bolt://10.113.8.4:8085\"\n",
    "username =\"neo4j\"\n",
    "password = \"password\"\n",
    "os.environ[\"NEO4J_URI\"] = url\n",
    "os.environ[\"NEO4J_USERNAME\"] = username\n",
    "os.environ[\"NEO4J_PASSWORD\"] = password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62feec32-e27d-40cc-98e7-9a7249f19e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"edl-idaas-rnd-platform-d5ae\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "728281d7-9212-4f6b-9f80-35fc013a4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatVertexAI(\n",
    "    model=\"gemini-1.5-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    max_retries=6,\n",
    "    stop=None,\n",
    "    # other params...\n",
    ")\n",
    "ollamallm = ChatOllama(base_url='http://10.113.8.4:8086',\n",
    "    model=\"llama3:latest\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a6e4c2-6571-46b8-ba62-b437eedf8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = \"/home/sridhanya_ganapathi_team_neustar/Assignment/data/images\"\n",
    "pdf_file_name = \"/home/sridhanya_ganapathi_team_neustar/Assignment/data/Health-Guard-Brochure-print.pdf\"\n",
    "Pdf_title=\"Health-Guard-Brochure-print.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920b982d-4113-48d7-bb37-035bddba62ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "image_pdf_folder_path = \"/home/sridhanya_ganapathi_team_neustar/Assignment/data/images\"\n",
    "pdf_file_name = \"/home/sridhanya_ganapathi_team_neustar/Assignment/data/Health-Guard-Brochure-print.pdf\"\n",
    "\n",
    "# Extract images, tables, and chunk text from a PDF file.\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=pdf_file_name,\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=pdf_folder_path,\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    extract_image_block_output_dir=image_pdf_folder_path,\n",
    "    extract_image_block_to_payload=False\n",
    ")\n",
    "\n",
    "# Categorize extracted elements from a PDF into tables and texts.\n",
    "tables = []\n",
    "texts = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        tables.append(str(element))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        texts.append(str(element))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daab5fec-b62c-4524-b2e7-5a62664e25ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0701000-f38f-4ea0-98ca-311332d3a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemini-1.0-pro-vision\"\n",
    "\n",
    "\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(\n",
    "    texts: List[str], tables: List[str], summarize_texts: bool = False\n",
    ") -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a ellaborate and concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_text)\n",
    "    empty_response = RunnableLambda(\n",
    "        lambda x: AIMessage(content=\"Error processing document\")\n",
    "    )\n",
    "    # Text summary chain\n",
    "    model = VertexAI(\n",
    "        temperature=0, model_name=MODEL_NAME, max_output_tokens=1024\n",
    "    ).with_fallbacks([empty_response])\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts:\n",
    "        if summarize_texts:\n",
    "            text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "        else:\n",
    "            text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "984479fb-5645-47e5-b3a7-01684d073c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    model = ChatVertexAI(model_name=\"gemini-pro-vision\", max_output_tokens=1024)\n",
    "\n",
    "    msg = model(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\n",
    "    If it's a table, extract all elements of the table.\n",
    "    If it's a graph, explain the findings in the graph.\n",
    "    Do not include any numbers that are not mentioned in the image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(image_pdf_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f051d95-9896-4dd2-9fbe-6237c44c9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all_texts = text_summaries + tables + image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bfef94f-e4c9-44ed-86cd-0332cf2a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = \".\".join(combined_all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3fa89ee-8e93-4a26-a73b-8c0bf6344f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c101d5a-1b4f-41ce-bd36-dfceaeb8ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2395, which is longer than the specified 1000\n",
      "Created a chunk of size 1125, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "texts_4k_token = text_splitter.split_text(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e914939-5953-4591-acd8-8d0b895dfea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_4k_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fbb7632-4215-4b6a-a6b7-6f021c29f602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.create_documents(texts_4k_token)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd447fbc-8ed6-49a2-878d-791dad015614",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_transformer = LLMGraphTransformer(llm=ollamallm, strict_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "41a0cd46-d02b-4286-bbb8-6ec0c306c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "028499a7-6eb7-4a5f-b4ad-59795279791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngraph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20460b82-3d59-405b-9cea-a1d5de52abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(0, len(docs)):\n",
    "        graph = llm_transformer.process_response(docs[i])\n",
    "        graph_documents.extend(graph)\n",
    "except Exception as e:\n",
    "    print(f\"Error processing document: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "753f05a4-2e28-480e-a18f-9b85c87e20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure graph_documents contains the correct objects\n",
    "processed_graph_documents = []\n",
    "for document in graph_documents:\n",
    "    if isinstance(document, tuple):\n",
    "        node = document[1]  # Assuming the Node object is the second element in the tuple\n",
    "        processed_graph_documents.append(node)\n",
    "    else:\n",
    "        processed_graph_documents.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48c185fb-d426-4f90-9a54-57719d92e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_document = GraphDocument(nodes=processed_graph_documents[0], relationships=processed_graph_documents[1], source=processed_graph_documents[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20377f1e-4ad6-408f-8e7b-873f3b9c574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add graph documents\n",
    "ngraph.add_graph_documents(\n",
    "    [graph_document],\n",
    "    baseEntityLabel=True,\n",
    "    include_source=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "11840c44-1241-4f60-9bd8-24473306acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings & LLM models\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embedding_dimension = 768\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Loading Gemini Pro Vision Model\n",
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")\n",
    "# Initializing embedding model\n",
    "EMBEDDING_MODEL =VertexAIEmbeddings(model_name=\"textembedding-gecko@003\", dimension=768) #TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "\n",
    "# Loading Gemini Pro Model\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-1.0-pro-vision\",\n",
    "    safety_settings={\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "605e73f7-4d2e-48a4-a6e5-59641baee312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parent_splitter = TokenTextSplitter(chunk_size=1048, chunk_overlap=24)\n",
    "child_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "parent_documents = parent_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c7501cad-3668-4aff-8cab-4bf4b09aacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3edd38d8-ee11-45e0-92f2-555c524ad230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3660\n",
      "3816\n",
      "4625\n",
      "4067\n",
      "4012\n",
      "4297\n",
      "375\n",
      "3301\n",
      "4059\n",
      "1437\n",
      "2135\n",
      "4179\n",
      "502\n",
      "3451\n",
      "2054\n"
     ]
    }
   ],
   "source": [
    "for i, parent in enumerate(parent_documents):\n",
    "    child_documents = child_splitter.split_documents([parent])\n",
    "    print(len(parent.page_content))\n",
    "    params = {\n",
    "        \"parent_text\": parent.page_content,\n",
    "        \"parent_id\": i,\n",
    "        \"parent_embedding\": EMBEDDING_MODEL.embed_query(parent.page_content), #EMBEDDING_MODEL.get_embeddings([parent.page_content])[0].values,\n",
    "        \"children\": [\n",
    "            {\n",
    "                \"text\": c.page_content,\n",
    "                \"id\": f\"{i}-{ic}\",\n",
    "                \"embedding\": EMBEDDING_MODEL.embed_query(c.page_content) #EMBEDDING_MODEL.get_embeddings([c.page_content])[0].values,\n",
    "            }\n",
    "            for ic, c in enumerate(child_documents)\n",
    "        ],\n",
    "    }\n",
    "     # Ingest data\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "    MERGE (p:Parent {id: $parent_id})\n",
    "    SET p.text = $parent_text\n",
    "    WITH p\n",
    "    CALL db.create.setVectorProperty(p, 'embedding', $parent_embedding)\n",
    "    YIELD node\n",
    "    WITH p \n",
    "    UNWIND $children AS child\n",
    "    MERGE (c:Child {id: child.id})\n",
    "    SET c.text = child.text\n",
    "    MERGE (c)<-[:HAS_CHILD]-(p)\n",
    "    WITH c, child\n",
    "    CALL db.create.setVectorProperty(c, 'embedding', child.embedding)\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\",\n",
    "        params,\n",
    "    )\n",
    "    # Create vector index for child\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('parent_document', \"\n",
    "            \"'Child', 'embedding', 768, 'cosine')\",\n",
    "            {},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n",
    "    # Create vector index for parents\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('typical_rag', \"\n",
    "            \"'Parent', 'embedding', 768, 'cosine')\",\n",
    "            {},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n",
    "# CALL db.index.vector.createNodeIndex('parent_document', 'Child', 'embedding', 768, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "48254711-e616-4996-b175-64aa02721004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest frequently asked questions  questions\n",
    "\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    \"\"\"Generating hypothetical questions about text.\"\"\"\n",
    "\n",
    "    questions: List[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Generated hypothetical questions based on  the information from the text\"\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9dd80f21-bdd1-447a-87e2-9ffb3705428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Use the given format to generate frequently asked questions from the \"\n",
    "                \"following input: {input}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "question_chain = questions_prompt | llm.with_structured_output(Questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f423bbd3-3acf-423b-8178-aff94ff12c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_chain = questions_prompt | llm.with_structured_output(Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "45770251-5d2c-430c-8769-3e285489cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, parent in enumerate(parent_documents):\n",
    "    resp = question_chain.invoke([parent.page_content])\n",
    "    if (resp != None):\n",
    "        questions = resp.questions\n",
    "        params = {\n",
    "            \"parent_id\": i,\n",
    "            \"questions\": [\n",
    "                {\"text\": q, \"id\": f\"{i}-{iq}\", \"embedding\": EMBEDDING_MODEL.embed_query(q)} #EMBEDDING_MODEL.get_embeddings([q])[0].values}\n",
    "                for iq, q in enumerate(questions)\n",
    "                if q\n",
    "            ],\n",
    "        }\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "        MERGE (p:Parent {id: $parent_id})\n",
    "        WITH p\n",
    "        UNWIND $questions AS question\n",
    "        CREATE (q:Question {id: question.id})\n",
    "        SET q.text = question.text\n",
    "        MERGE (q)<-[:HAS_QUESTION]-(p)\n",
    "        WITH q, question\n",
    "        CALL db.create.setVectorProperty(q, 'embedding', question.embedding)\n",
    "        YIELD node\n",
    "        RETURN count(*)\n",
    "        \"\"\",\n",
    "            params,\n",
    "        )\n",
    "        # Create vector index #CALL db.index.vector.createNodeIndex\n",
    "        try:\n",
    "            graph.query(\n",
    "                \"CALL db.index.vector.createNodeIndex('hypothetical_questions', \"\n",
    "                \"'Question', 'embedding', 768, 'cosine')\",\n",
    "                {},\n",
    "            )\n",
    "        except ClientError:  # already exists\n",
    "            pass\n",
    "#CALL db.index.vector.createNodeIndex('hypothetical_questions', 'Question', 'embedding', 768, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e072366e-3aba-421d-8b5d-30bb19499565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ingest summaries\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            (\"Generate a summary of the following input: {question}\\n\" \"Summary:\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summary_chain = summary_prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c26cb53b-15b4-476c-b33b-e1f1ed660f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, parent in enumerate(parent_documents):\n",
    "    summary = summary_chain.invoke({\"question\": parent.page_content}).content\n",
    "    params = {\n",
    "        \"parent_id\": i,\n",
    "        \"summary\": summary,\n",
    "        \"embedding\": EMBEDDING_MODEL.embed_query(summary)# EMBEDDING_MODEL.get_embeddings([summary])[0].values,\n",
    "    }\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "    MERGE (p:Parent {id: $parent_id})\n",
    "    MERGE (p)-[:HAS_SUMMARY]->(s:Summary)\n",
    "    SET s.text = $summary\n",
    "    WITH s\n",
    "    CALL db.create.setVectorProperty(s, 'embedding', $embedding)\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\",\n",
    "        params,\n",
    "    )\n",
    "    # Create vector index\n",
    "    #CALL db.index.vector.createNodeIndex('typical_rag', 'Parent', 'embedding', 768, 'cosine')\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('summary', \"\n",
    "            \"'Summary', 'embedding', 768, 'cosine')\",\n",
    "            {},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n",
    "# CALL db.index.vector.createNodeIndex('summary', 'Summary', 'embedding', 768, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a2f983db-aafb-435b-9c75-4fb3aa6c61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt_template = '''You are a helpful assistant and an expert in natural language processing and specialize in open set discovery intent classification. This task involves assigning textual utterances to specific intents,\n",
    "                some of which are pre-defined (known) and others are not (unknown). \\nAs an expert in open set discovery intent classification,\n",
    "                your task is to provide a list of  pairs of utterances and intents that are the most difficult to classify. These pairs should consist of one utterance and its corresponding intent. Make sure that the final list contains high variety of intents. The hard samples you propose need to be extracted from the original list.\n",
    "                given the following text: {Text}\n",
    "                To select the most difficult pairs, consider the following criteria:\n",
    "                1. Ambiguity: Choose utterances that can be interpreted in multiple ways, making it challenging to determine the correct intent.\n",
    "                2. Contextual complexity: Select pairs where the intent is highly dependent on the context provided in the utterance or surrounding dialogue.\n",
    "                3. Lack of explicit keywords: Include pairs with utterances that do not contain obvious keywords or explicit indicators of the intent.\n",
    "                4. Similarities among intents: Include pairs where intents have overlapping or similar meanings, making it challenging to differentiate between them.\n",
    "\n",
    "                Once you have selected the difficult pairs, and have acquired sufficient context about the problem, provide a detailed prompt for an AI language model to solve the task of open set intent discovery, maximizing the model's performance in the task. Provide effective guidelines about how to solve the task in the prompt. \n",
    "\n",
    "                EXAMPLES:\n",
    "                \n",
    "                Text : please book an appointment with a Dermatologist \n",
    "                Intent: book/create/appointment\n",
    "                Text :  how much total coverage do I have left \n",
    "                Intent: coverage/calculate/pendingamount\n",
    "        \n",
    "                Else \n",
    "                Fallback Intent: query \n",
    "               '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8a67d36d-e9d6-4e4f-91dd-f27c2d12971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "cllm = ChatVertexAI(\n",
    "    model=\"gemini-1.0-pro-vision\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    max_retries=6,\n",
    "    stop=None,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "00bbf373-fef7-4789-a55c-0d3107843f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"please book an appointment with a Doctor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8631a30a-1f1e-49b4-b781-92ac9cdc660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(intent_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7e5dd20b-b1a3-44c9-ab40-26ede1a8a33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='After analyzing the provided text, I\\'ve extracted the most challenging pairs of utterances and intents that meet the criteria for ambiguity, contextual complexity, lack of explicit keywords, and similarities among intents. Here are some examples:\\n\\n1. Text: \"I need to reschedule my appointment.\"\\nIntent: book/create/appointment (or possibly query)\\n\\nThis pair is challenging because it can be interpreted as either a request to reschedule an existing appointment or a query about the possibility of rescheduling.\\n\\n2. Text: \"What\\'s the best way to get rid of acne?\"\\nIntent: query (or possibly treatment/consultation)\\n\\nThis pair is difficult because the utterance could be asking for general advice on how to treat acne, rather than seeking a specific consultation with a doctor.\\n\\n3. Text: \"I\\'m experiencing some weird symptoms.\"\\nIntent: report/symptom (or possibly query)\\n\\nThis pair is challenging because the utterance could be reporting unusual symptoms or simply expressing concern and seeking guidance.\\n\\n4. Text: \"Can I get a referral to a specialist?\"\\nIntent: request/referral (or possibly query)\\n\\nThis pair is difficult because the utterance could be requesting a referral for a specific medical condition or simply inquiring about the process of getting a referral.\\n\\n5. Text: \"I\\'m having trouble with my insurance.\"\\nIntent: issue/insurance (or possibly query)\\n\\nThis pair is challenging because the utterance could be reporting an issue with their insurance coverage or seeking guidance on how to resolve the problem.\\n\\n6. Text: \"What\\'s the difference between a primary care physician and a specialist?\"\\nIntent: query (or possibly information/education)\\n\\nThis pair is difficult because the utterance is asking for general information about different types of medical professionals, rather than seeking a specific consultation or appointment.\\n\\n7. Text: \"I need to get my prescription filled.\"\\nIntent: request/prescription (or possibly query)\\n\\nThis pair is challenging because the utterance could be requesting a refill on an existing prescription or simply inquiring about the process of getting a new prescription.\\n\\n8. Text: \"Can you recommend a good doctor?\"\\nIntent: request/recommendation (or possibly query)\\n\\nThis pair is difficult because the utterance could be seeking a general recommendation for a doctor or asking for specific advice on how to find a suitable healthcare provider.\\n\\nTo maximize the performance of an AI language model in open set intent discovery, I provide the following prompt:\\n\\n**Task:** Classify the given text into one of the predefined intents (book/create/appointment, coverage/calculate/pendingamount, etc.) or identify it as an unknown intent that requires further exploration.\\n\\n**Guidelines:**\\n\\n1. **Readability**: Start by reading the entire utterance to understand its context and meaning.\\n2. **Intent identification**: Identify the most likely intent based on the utterance\\'s content, syntax, and semantics.\\n3. **Contextual analysis**: Analyze the utterance\\'s context, including any surrounding dialogue or relevant information, to refine your intent classification.\\n4. **Ambiguity resolution**: If the utterance is ambiguous, consider alternative interpretations and weigh their likelihood against each other.\\n5. **Intent similarity**: Be aware of intents with similar meanings and carefully differentiate between them.\\n6. **Unknown intent detection**: If the utterance does not fit any predefined intent, identify it as an unknown intent that requires further exploration.\\n\\n**Evaluation metrics:**\\n\\n1. Accuracy: Measure the proportion of correctly classified utterances.\\n2. F1-score: Calculate the harmonic mean of precision and recall to evaluate the model\\'s performance in identifying both known and unknown intents.\\n3. Mean Average Precision (MAP): Evaluate the model\\'s ability to rank intents in order of their relevance to the given text.\\n\\nBy following these guidelines and using effective evaluation metrics, you can develop a robust AI language model that excels in open set intent discovery.', response_metadata={'model': 'llama3:latest', 'created_at': '2024-08-26T14:29:10.011939614Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 20644869426, 'load_duration': 23720868, 'prompt_eval_count': 390, 'prompt_eval_duration': 477298000, 'eval_count': 780, 'eval_duration': 20097431000}, id='run-62919f34-cd82-4562-b35c-c88218c06b43-0')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = intent_prompt | ollamallm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"Text\": input_text\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2839d13-4c89-4c44-9540-3a2ba9ec0213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bc12a-1b2d-4e5a-ad12-33d5add33bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
