{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273c721-e075-44e9-8d6e-37b6b1cd708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --user --quiet google-cloud-aiplatform datasets backoff multiprocess gcsfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa967e6b-44b4-4f99-b327-fdbeee82895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4d3b4f33-bc98-40c8-b5ce-5f84f97c8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "import io\n",
    "# Data Handling and Processing\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "\n",
    "# Google Cloud Libraries\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerativeModel,\n",
    "    GenerationConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "from vertexai.preview.tuning import sft\n",
    "\n",
    "# Multiprocessing\n",
    "import multiprocess as mp\n",
    "from tqdm import tqdm\n",
    "import backoff\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ede0a1-111c-413c-af07-0f1ea04cbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"train_40k.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c30a3e7-844e-4787-8a76-22bd265ae5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c76dd86c-c22b-4e07-b8e2-b4f0fe11653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"productId\",\n",
    "    \"Title\",\n",
    "    \"userId\",\n",
    "    \"Helpfulness\",\n",
    "    \"Score\",\n",
    "    \"Time\",\n",
    "    \"Cat1\",\n",
    "]  # List of columns to drop\n",
    "data = data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47edb266-5ca2-4cb5-a11c-dea1db19cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cat2_classes = data[\"Cat2\"].unique()\n",
    "print(len(cat2_classes))\n",
    "\n",
    "cat3_classes = data[\"Cat3\"].unique()\n",
    "print(len(cat3_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a071e279-a591-4852-8972-dfb27cf555bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the test data into another CSV file\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "test_df.to_csv(\"train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32f809fe-f171-4aca-afe2-20bc4cf458ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"edl-idaas-rnd-platform-d5ae\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40fc9a5-5a9a-4f1d-b77c-8123be1f2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"sridhanya_edl-idaas-rnd-platform\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb8ece6-1c1e-4744-abe6-122f048a28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b533027e-9300-4bb9-8779-e6136d5a6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_hdlr(details) -> None:\n",
    "    \"\"\"\n",
    "    Handles backoff events.\n",
    "\n",
    "    Args:\n",
    "        details: A dictionary containing information about the backoff event.\n",
    "    \"\"\"\n",
    "    print(f\"Backing off {details['wait']:.1f} seconds after {details['tries']} tries\")\n",
    "\n",
    "\n",
    "def log_error(msg: str, *args: Any) -> None:\n",
    "    \"\"\"\n",
    "    Logs an error message and raises an exception.\n",
    "\n",
    "    Args:\n",
    "        msg: The error message.\n",
    "        *args: Additional arguments to be passed to the logger.\n",
    "    \"\"\"\n",
    "    mp.get_logger().error(msg, *args)\n",
    "    raise Exception(msg)\n",
    "\n",
    "\n",
    "def handle_exception_threading(f: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    A decorator that handles exceptions in a threaded environment.\n",
    "\n",
    "    Args:\n",
    "        f: The function to decorate.\n",
    "\n",
    "    Returns:\n",
    "        The decorated function.\n",
    "    \"\"\"\n",
    "\n",
    "    def applicator(*args: Any, **kwargs: Any) -> Any:\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            log_error(traceback.format_exc())\n",
    "\n",
    "    return applicator\n",
    "\n",
    "\n",
    "@handle_exception_threading\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, ResourceExhausted, max_tries=30, on_backoff=backoff_hdlr\n",
    ")\n",
    "def _predict_message(message: str, model: GenerativeModel) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Predict messages\n",
    "\n",
    "    Args:\n",
    "        message: The message to predict.\n",
    "        model: The GenerativeModel to use for prediction.\n",
    "\n",
    "    Returns:\n",
    "        The predicted message, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    response = model.generate_content([message], stream=False)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def batch_predict(\n",
    "    messages: List[str], model: GenerativeModel, max_workers: int = 4\n",
    ") -> List[Optional[str]]:\n",
    "    \"\"\"\n",
    "    Predicts the classes for a list of messages\n",
    "\n",
    "    Args:\n",
    "        - messages: list of all messages to predict\n",
    "        - model: model to use for predicting.\n",
    "        - max_workers: number of workers to use for parallel predictions\n",
    "\n",
    "    Returns:\n",
    "        - list of predicted labels\n",
    "\n",
    "    \"\"\"\n",
    "    predictions = list()\n",
    "    with ThreadPoolExecutor(max_workers) as pool:\n",
    "        partial_func = partial(_predict_message, model=model)\n",
    "        for message in tqdm(pool.map(partial_func, messages), total=len(messages)):\n",
    "            predictions.append(message)\n",
    "            pass\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6fffa5d-7749-4b93-b4f7-7bdb0f2bdc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexAIExperimentManager:\n",
    "    \"\"\"\n",
    "    A class for managing experiments and runs in Vertex AI.\n",
    "    This class encapsulates the functionality for creating experiments, logging runs,\n",
    "    and retrieving experiment data in Vertex AI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, project: str, location: str):\n",
    "        self.project = project\n",
    "        self.location = location\n",
    "        self.current_experiment = None\n",
    "\n",
    "    def init_experiment(\n",
    "        self, experiment_name: str, experiment_description: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Initialize or switch to a specific experiment.\"\"\"\n",
    "        self.current_experiment = experiment_name\n",
    "        aiplatform.init(\n",
    "            experiment=experiment_name,\n",
    "            experiment_description=experiment_description,\n",
    "            experiment_tensorboard=False,\n",
    "            project=self.project,\n",
    "            location=self.location,\n",
    "        )\n",
    "\n",
    "    def create_experiment(\n",
    "        self, experiment_name: str, experiment_description: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Create an Experiment on Vertex AI Experiments\"\"\"\n",
    "        self.init_experiment(experiment_name, experiment_description)\n",
    "\n",
    "    def log_run(\n",
    "        self, run_name: str, params: Dict[str, Any], metrics: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Log experiment run data to Vertex AI Experiments.\"\"\"\n",
    "        if not self.current_experiment:\n",
    "            raise ValueError(\"No experiment initialized. Call init_experiment first.\")\n",
    "\n",
    "        aiplatform.start_run(run=run_name)\n",
    "        aiplatform.log_params(params)\n",
    "        aiplatform.log_metrics(metrics)\n",
    "        aiplatform.end_run()\n",
    "\n",
    "    def get_experiments_data_frame(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Retrieve a DataFrame of experiment data from Vertex AI Experiments.\"\"\"\n",
    "        if not self.current_experiment:\n",
    "            raise ValueError(\"No experiment initialized. Call init_experiment first.\")\n",
    "\n",
    "        return aiplatform.get_experiment_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "267bef06-c987-4ff7-98aa-47759a6436e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gemini_messages(\n",
    "    text: str, label: str, system_prompt: Optional[str] = None\n",
    ") -> dict:\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.extend(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": Text},\n",
    "            {\"role\": \"model\", \"content\": Cat3},\n",
    "        ]\n",
    "    )\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "def prepare_tuning_dataset_from_df(\n",
    "    tuning_df: pd.DataFrame, system_prompt: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares a tuning dataset from a pandas DataFrame for Gemini fine-tuning.\n",
    "    Args:\n",
    "        tuning_df: A pandas DataFrame with columns \"text\" and \"label_text\".\n",
    "        system_prompt: An optional system prompt for zero-shot learning.\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the data in the Gemini tuning format.\n",
    "    \"\"\"\n",
    "    tuning_dataset = [\n",
    "        create_gemini_messages(row[\"Text\"], row[\"Cat3\"], system_prompt)\n",
    "        for _, row in tuning_df.iterrows()\n",
    "    ]\n",
    "    return pd.DataFrame(tuning_dataset)\n",
    "\n",
    "\n",
    "def convert_tuning_dataset_from_automl_csv(\n",
    "    automl_gcs_csv_path: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    partition: str = \"training\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts an AutoML CSV dataset for text classification to the Gemini tuning format.\n",
    "    Args:\n",
    "        automl_gcs_csv_path: The GCS path to the AutoML CSV dataset.\n",
    "        system_prompt: The instructions to the model.\n",
    "        partition: The partition to extract from the dataset (e.g., \"training\", \"validation\", \"test\"). Defaults to \"training\".\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the data in the Gemini tuning format.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(automl_gcs_csv_path, names=[\"partition\", \"Text\", \"Cat3\"])\n",
    "    df_automl = df.loc[df[\"partition\"] == partition]\n",
    "    gemini_dataset = [\n",
    "        create_gemini_messages(row[\"Text\"], row[\"Cat3\"], system_prompt)\n",
    "        for _, row in df_automl.iterrows()\n",
    "    ]\n",
    "    return pd.DataFrame(gemini_dataset)\n",
    "\n",
    "\n",
    "def convert_tuning_dataset_from_automl_jsonl(\n",
    "    project_id: str,\n",
    "    automl_gcs_jsonl_path: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    partition: str = \"training\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts an AutoML JSONL dataset for text classification to the Gemini tuning format.\n",
    "    Args:\n",
    "        automl_gcs_jsonl_path: The GCS path to the AutoML JSONL dataset for text classification.\n",
    "        system_prompt: The instructions to the model.\n",
    "        partition: The partition to extract from the dataset (e.g., \"training\", \"validation\", \"test\"). Defaults to \"training\".\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the data in the Gemini tuning format.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    gcs_file_system = gcsfs.GCSFileSystem(project=project_id)\n",
    "    with gcs_file_system.open(automl_gcs_jsonl_path) as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            processed_data.append(\n",
    "                {\n",
    "                    \"Cat3\": data[\"classificationAnnotation\"][\"displayName\"],\n",
    "                    \"Text\": data[\"textContent\"],\n",
    "                    \"partition\": data[\"dataItemResourceLabels\"][\n",
    "                        \"aiplatform.googleapis.com/ml_use\"\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    df_automl = df.loc[df[\"partition\"] == partition]\n",
    "    gemini_dataset = [\n",
    "        create_gemini_messages(row[\"Text\"], row[\"Cat3\"], system_prompt)\n",
    "        for _, row in df_automl.iterrows()\n",
    "    ]\n",
    "    return pd.DataFrame(gemini_dataset)\n",
    "\n",
    "\n",
    "def validate_gemini_tuning_jsonl(gcs_jsonl_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Validates a JSONL file on Google Cloud Storage against the Gemini tuning format.\n",
    "\n",
    "    Args:\n",
    "        gcs_jsonl_path: The GCS path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries representing the errors found in the file.\n",
    "        Each dictionary has the following structure:\n",
    "        {\n",
    "            \"error_type\": \"Error description\",\n",
    "            \"row_index\": The index of the row where the error occurred,\n",
    "            \"message\": The error message\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    errors = []\n",
    "    storage_client = storage.Client()\n",
    "    blob = storage.Blob.from_string(uri=gcs_jsonl_path, client=storage_client)\n",
    "\n",
    "    with blob.open(\"r\") as f:\n",
    "        for row_index, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                # Check for the presence of the \"messages\" key\n",
    "                if \"messages\" not in data:\n",
    "                    errors.append(\n",
    "                        {\n",
    "                            \"error_type\": \"Missing 'messages' key\",\n",
    "                            \"row_index\": row_index,\n",
    "                            \"message\": f\"Row {row_index} is missing the 'messages' key.\",\n",
    "                        }\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                messages = data[\"messages\"]\n",
    "                # Check if \"messages\" is a list\n",
    "                if not isinstance(messages, list):\n",
    "                    errors.append(\n",
    "                        {\n",
    "                            \"error_type\": \"Invalid 'messages' type\",\n",
    "                            \"row_index\": row_index,\n",
    "                            \"message\": f\"Row {row_index}: 'messages' is not a list.\",\n",
    "                        }\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Validate each message in the \"messages\" list\n",
    "                for message_index, message in enumerate(messages):\n",
    "                    if not isinstance(message, dict):\n",
    "                        errors.append(\n",
    "                            {\n",
    "                                \"error_type\": \"Invalid message format\",\n",
    "                                \"row_index\": row_index,\n",
    "                                \"message\": f\"\"\"Row {row_index},\n",
    "                            message {message_index}: Message is not a dictionary.\"\"\",\n",
    "                            }\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    # Check for required keys in each message dictionary\n",
    "                    if \"role\" not in message or \"content\" not in message:\n",
    "                        errors.append(\n",
    "                            {\n",
    "                                \"error_type\": \"Missing 'role' or 'content' key\",\n",
    "                                \"row_index\": row_index,\n",
    "                                \"message\": f\"Row {row_index}, message {message_index}: \"\n",
    "                                \"Missing 'role' or 'content' key.\",\n",
    "                            }\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    # Check for valid role values\n",
    "                    if message[\"role\"] not in [\"system\", \"user\", \"model\"]:\n",
    "                        errors.append(\n",
    "                            {\n",
    "                                \"error_type\": \"Invalid 'role' value\",\n",
    "                                \"row_index\": row_index,\n",
    "                                \"message\": f\"\"\"Row {row_index}, message {message_index}:\n",
    "                            Invalid 'role' value. Expected 'system', 'user', or 'model'.\"\"\",\n",
    "                            }\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                errors.append(\n",
    "                    {\n",
    "                        \"error_type\": \"JSON Decode Error\",\n",
    "                        \"row_index\": row_index,\n",
    "                        \"message\": f\"Row {row_index}: JSON decoding error: {e}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fba6e88-a675-4f0a-9010-deee452e9aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b3cce57-e337-429e-a840-96cc79e6ab5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3dd0850-e515-46bd-97eb-0ed40e84ea9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cat2</th>\n",
       "      <th>Cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14307</th>\n",
       "      <td>The concept of this toy is good. However, if y...</td>\n",
       "      <td>dogs</td>\n",
       "      <td>toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17812</th>\n",
       "      <td>This dryer ruined my hair!!! At first, after I...</td>\n",
       "      <td>hair care</td>\n",
       "      <td>styling tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11020</th>\n",
       "      <td>Much to my surprise after a year of waiting th...</td>\n",
       "      <td>novelty gag toys</td>\n",
       "      <td>miniatures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15158</th>\n",
       "      <td>The tree is beautiful but upon arrival when I ...</td>\n",
       "      <td>fresh flowers live indoor plants</td>\n",
       "      <td>live indoor plants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>Watchmaker offered to install a new battery in...</td>\n",
       "      <td>household supplies</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "14307  The concept of this toy is good. However, if y...   \n",
       "17812  This dryer ruined my hair!!! At first, after I...   \n",
       "11020  Much to my surprise after a year of waiting th...   \n",
       "15158  The tree is beautiful but upon arrival when I ...   \n",
       "24990  Watchmaker offered to install a new battery in...   \n",
       "\n",
       "                                   Cat2                Cat3  \n",
       "14307                              dogs                toys  \n",
       "17812                         hair care       styling tools  \n",
       "11020                  novelty gag toys          miniatures  \n",
       "15158  fresh flowers live indoor plants  live indoor plants  \n",
       "24990                household supplies             unknown  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bee6f4b8-361a-4417-972c-b6a0d0304e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cat2\n",
       "personal care         2294\n",
       "dogs                  2092\n",
       "nutrition wellness    1780\n",
       "health care           1614\n",
       "cats                  1428\n",
       "                      ... \n",
       "produce                 33\n",
       "baby food               32\n",
       "sauces dips             32\n",
       "meat seafood            25\n",
       "small animals           22\n",
       "Name: count, Length: 64, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Cat2.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901ca963-23c7-436e-836e-101365e47972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cat3\n",
       "unknown                 1832\n",
       "shaving hair removal    1238\n",
       "vitamins supplements    1071\n",
       "board games              738\n",
       "styling tools            670\n",
       "                        ... \n",
       "fruit gifts                1\n",
       "foie gras p t s            1\n",
       "children s                 1\n",
       "aprons smocks              1\n",
       "pork                       1\n",
       "Name: count, Length: 451, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Cat3.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a745bd3c-7201-4d26-9295-c80e21e357c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = test_df.groupby(\"Cat3\").filter(lambda x: len(x) > 1)\n",
    "val, test = train_test_split(\n",
    "    filtered_df, test_size=0.75, shuffle=True, stratify=filtered_df[\"Cat3\"], random_state=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6554416b-6f5e-4a97-9df5-0405bc81a1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1982, 3)\n",
      "(5946, 3)\n"
     ]
    }
   ],
   "source": [
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d108500-8800-49c8-8141-8c02e90c80ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cat3\n",
       "unknown                   108\n",
       "shaving hair removal       82\n",
       "vitamins supplements       61\n",
       "board games                47\n",
       "styling tools              45\n",
       "                         ... \n",
       "cakes                       1\n",
       "basic life skills toys      1\n",
       "fruit leather               1\n",
       "joggers                     1\n",
       "aquarium heaters            1\n",
       "Name: count, Length: 304, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.Cat3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f61b0b6b-69d3-4769-a184-8507b035a3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cat3\n",
       "unknown                 322\n",
       "shaving hair removal    245\n",
       "vitamins supplements    183\n",
       "board games             139\n",
       "styling tools           135\n",
       "                       ... \n",
       "washcloths towels         1\n",
       "snack gifts               1\n",
       "money banks               1\n",
       "crackers biscuits         1\n",
       "stimulants                1\n",
       "Name: count, Length: 320, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Cat3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8e4ce16-f307-46af-ae22-8b52486072c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_postprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the predicted class label string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The predicted class label string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned class label string.\n",
    "    \"\"\"\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "def evaluate_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    target_column: str = \"label_text\",\n",
    "    predictions_column: str = \"predicted_labels\",\n",
    "    postprocessing: bool = True,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Batch evaluation of predictions, returns a dictionary with the metric.\n",
    "\n",
    "    Args:\n",
    "       - df (pandas.DataFrame):  a pandas dataframe with two mandatory columns, a target column with\n",
    "       the actual true values, and a predictions column with the predicted values.\n",
    "       - target_column (str): column name with the actual ground truth values\n",
    "       - predictions_column (str): column name with the model predictions\n",
    "       - postprocessing (bool): whether to apply postprocessing to predictions.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    if postprocessing:\n",
    "        df[predictions_column] = df[predictions_column].apply(\n",
    "            predictions_postprocessing\n",
    "        )\n",
    "\n",
    "    y_true = df[target_column]\n",
    "    y_pred = df[predictions_column]\n",
    "\n",
    "    metrics_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    overall_macro_f1_score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    overall_micro_f1_score = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    weighted_precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    weighted_recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": metrics_report[\"accuracy\"],\n",
    "        \"weighted precision\": weighted_precision,\n",
    "        \"weighted recall\": weighted_recall,\n",
    "        \"macro f1\": overall_macro_f1_score,\n",
    "        \"micro f1\": overall_micro_f1_score,\n",
    "    }\n",
    "\n",
    "    categories = [\"business\", \"sport\", \"politics\", \"tech\", \"entertainment\"]\n",
    "    for category in categories:\n",
    "        if category in metrics_report:\n",
    "            metrics[f\"{category}_f1_score\"] = metrics_report[category][\"f1-score\"]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f818802-24a0-4872-aa71-1ef2ed7e2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"sridhanya-classification\"  # @param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61ea1ffe-8ac2-4ef8-8a46-bef33c45cd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-7cec03b9-cdb7-4249-bcf7-a3448e57639a\" href=\"#view-view-vertex-resource-7cec03b9-cdb7-4249-bcf7-a3448e57639a\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-7cec03b9-cdb7-4249-bcf7-a3448e57639a');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/sridhanya-classification/runs?project=edl-idaas-rnd-platform-d5ae');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/sridhanya-classification/runs?project=edl-idaas-rnd-platform-d5ae', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_manager = VertexAIExperimentManager(project=PROJECT_ID, location=LOCATION)\n",
    "experiment_manager.create_experiment(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    experiment_description=\"Fine-tuning Gemini 1.0 Pro for text classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af5fa203-494d-4e05-b752-82cc877ec674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Evaluation dataframe to store the predictions from all the experiments.\n",
    "df_evals = val[0:50].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92d2cd8d-d548-4deb-abe9-ba8601af6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the classes into a string\n",
    "classes_list_str = \"\\n- \".join(cat3_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4a224a3-d0d0-4d69-b124-665f6afde955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first index for each unique category in 'Cat3'\n",
    "first_indices = train_df.drop_duplicates(subset='Cat3').index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e5ae639-6448-4e76-8d2f-7491f7c0bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_zero_shot = \"\"\"TASK:\n",
    "Classify the text into ONLY one of the following classes .\n",
    "\n",
    "CLASSES:\n",
    "- {classes_list_str}\n",
    "\n",
    "\n",
    "INSTRUCTIONS\n",
    "- Respond with ONLY one class.\n",
    "- You MUST use the exact word from the list above.\n",
    "- DO NOT create or use any other classes.\n",
    "- CAREFULLY analyze the text before choosing the best-fitting category.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10058039-13e0-4be6-8f9f-602f05683723",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_few_shot = f\"\"\"TASK:\n",
    "Classify the text into ONLY one of the following classes [business, entertainment, politics, sport, tech].\n",
    "\n",
    "CLASSES:\n",
    "- {classes_list_str}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Respond with ONLY one class.\n",
    "- You MUST use the exact word from the list above.\n",
    "- DO NOT create or use any other classes.\n",
    "- CAREFULLY analyze the text before choosing the best-fitting category.\n",
    "\n",
    "EXAMPLES:\n",
    "- EXAMPLE 1:\n",
    "    <user>\n",
    "    {train_df.loc[first_indices[0]].Text}\n",
    "    <model>\n",
    "    {train_df.loc[first_indices[0]].Cat3}\n",
    "\n",
    "- EXAMPLE 2:\n",
    "    <user>\n",
    "    {train_df.loc[first_indices[1]].Text}\n",
    "    <model>\n",
    "    {train_df.loc[first_indices[1]].Cat3}\n",
    "\n",
    "- EXAMPLE 3:\n",
    "    <user>\n",
    "    {train_df.loc[first_indices[2]].Text}\n",
    "    <model>\n",
    "    {train_df.loc[first_indices[21]].Cat3}\n",
    "\n",
    "- EXAMPLE 4:\n",
    "    <user>\n",
    "    {train_df.loc[first_indices[3]].Text}\n",
    "    <model>\n",
    "    {train_df.loc[first_indices[31]].Cat3}\n",
    "\n",
    "- EXAMPLE 4:\n",
    "    <user>\n",
    "    {train_df.loc[first_indices[4]].Text}\n",
    "    <model>\n",
    "    {train_df.loc[first_indices[4]].Cat3}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cd53606-29a2-4936-9ce7-5e19a6d28110",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_output_tokens=10, temperature=0)\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2048640d-a012-441b-a735-4ead564a8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_pro_1_model_zero = GenerativeModel(\n",
    "    \"gemini-1.0-pro-002\",  # e.g. gemini-1.5-pro-001, gemini-1.5-flash-001\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4bc870b-d7df-42f0-90d1-1cbe7f34dda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  9.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the list of messages to predict\n",
    "messages_to_predict = val[\"Text\"][0:50].to_list()\n",
    "# Compute the preictions\n",
    "predictions_zero_shot = batch_predict(\n",
    "    messages=messages_to_predict, model=gem_pro_1_model_zero, max_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9c272b55-4da4-4973-b20c-b4c5f565c367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals[\"gem1.0-zero-shot_predictions\"] = predictions_zero_shot\n",
    "len(predictions_zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26851424-7d58-4f5c-8333-456beaeb5ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0,\n",
       " 'weighted precision': 0.0,\n",
       " 'weighted recall': 0.0,\n",
       " 'macro f1': 0.0,\n",
       " 'micro f1': 0.0}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Evaluation Metrics for zero-shot prompt\n",
    "metrics_zero_shot = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"Cat3\",\n",
    "    predictions_column=\"gem1.0-zero-shot_predictions\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "43899e58-18a7-43af-9550-ea53306eb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Few-Shot, and other prompts/possibilities\n",
    "gem_pro_1_model_few = GenerativeModel(\n",
    "    \"gemini-1.0-pro-002\",\n",
    "    system_instruction=[system_prompt_few_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc7d4044-1075-4f76-bea9-5690e00d0f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.28it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_few_shot = batch_predict(\n",
    "    messages=messages_to_predict, model=gem_pro_1_model_few\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8486ee96-5b8f-4ca9-821e-f3ff9428b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals[\"gem1.0-few-shot_predictions\"] = predictions_few_shot\n",
    "len(predictions_few_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a422445c-66aa-4e64-8e32-3ec00d8abec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/llm/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.2,\n",
       " 'weighted precision': 0.22819047619047617,\n",
       " 'weighted recall': 0.2,\n",
       " 'macro f1': 0.12716262975778547,\n",
       " 'micro f1': 0.2,\n",
       " 'business_f1_score': 0.0}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Evaluation Metrics for few-shot prompt\n",
    "metrics_few_shot = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"Cat3\",\n",
    "    predictions_column=\"gem1.0-few-shot_predictions\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aa1ce220-0a57-4d54-8439-2e1a34a4f10d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tuning_gemini_df \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_tuning_dataset_from_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuning_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt_zero_shot\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m, in \u001b[0;36mprepare_tuning_dataset_from_df\u001b[0;34m(tuning_df, system_prompt)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_tuning_dataset_from_df\u001b[39m(\n\u001b[1;32m     17\u001b[0m     tuning_df: pd\u001b[38;5;241m.\u001b[39mDataFrame, system_prompt: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Prepares a tuning dataset from a pandas DataFrame for Gemini fine-tuning.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        A pandas DataFrame containing the data in the Gemini tuning format.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     tuning_dataset \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_gemini_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtuning_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(tuning_dataset)\n",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_tuning_dataset_from_df\u001b[39m(\n\u001b[1;32m     17\u001b[0m     tuning_df: pd\u001b[38;5;241m.\u001b[39mDataFrame, system_prompt: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Prepares a tuning dataset from a pandas DataFrame for Gemini fine-tuning.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        A pandas DataFrame containing the data in the Gemini tuning format.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     tuning_dataset \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 28\u001b[0m         create_gemini_messages(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_text\u001b[39m\u001b[38;5;124m\"\u001b[39m], system_prompt)\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tuning_df\u001b[38;5;241m.\u001b[39miterrows()\n\u001b[1;32m     30\u001b[0m     ]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(tuning_dataset)\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.11/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.11/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "tuning_gemini_df = prepare_tuning_dataset_from_df(\n",
    "    tuning_df=train_df, system_prompt=system_prompt_zero_shot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086222a5-790b-4324-9a9c-7f8b7b3250f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
